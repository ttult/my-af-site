import asyncio
import time
import os
import sys
from datetime import datetime
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError
from bs4 import BeautifulSoup # â˜…è¿½åŠ ï¼šBeautifulSoupã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

# å®Ÿè¡Œæ—¥ã®æ—¥ä»˜ã‚’å–å¾—ã—ã€URLã«çµ„ã¿è¾¼ã‚€
TODAY_DATE_STR = datetime.now().strftime("%Y-%m-%d")
ACCESS_DELAY = 5 # ç§’: ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚ã®å¾…æ©Ÿæ™‚é–“

# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆURLã‚’DLsite Maniaxã®å½“æ—¥æ–°ç€ä½œå“ä¸€è¦§ãƒšãƒ¼ã‚¸ã«è¨­å®š
HOME_URL = f"https://www.dlsite.com/maniax/new/=/date/{TODAY_DATE_STR}/"

# DLsiteã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼å®šç¾©
# å®Ÿè¡Œçµæœã§ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ãŒã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸãŸã‚ã€ä»Šå›ã¯ãƒ‘ãƒ¼ã‚¹å‡¦ç†ã«é›†ä¸­ã—ã¾ã™ãŒã€ã‚³ãƒ¼ãƒ‰ã¯æ®‹ã—ã¾ã™ã€‚
LANGUAGE_SELECTOR = '#language_select_ja' 
AGE_CONFIRM_SELECTOR = '//*[@id="age_confirm"]/div/div/div[2]/button[2]' 

OUTPUT_FILENAME = "dlsite_new_products_final.html"

# ==========================================================
# ğŸš€ Playwrightã«ã‚ˆã‚‹éåŒæœŸã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•°ï¼ˆå¤‰æ›´ãªã—ï¼‰
# ==========================================================
async def scrape_dlsite_new_products(target_url: str, headless_mode: bool = True):
    """
    Playwright (Chromium) ã‚’ä½¿ç”¨ã—ã¦DLsiteã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€è¨€èªé¸æŠã¨å¹´é½¢ç¢ºèªã‚’å‡¦ç†ã—ã¾ã™ã€‚
    """
    print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆURL: {target_url}")
    print(f"--- Playwright ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•ä¸­ (Headless: {headless_mode}) ---")

    async with async_playwright() as p:
        # browserã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¨­å®š
        browser = await p.chromium.launch(
            headless=headless_mode,
            args=['--no-sandbox', '--disable-blink-features=AutomationControlled'],
            timeout=90000
        )
        page = await browser.new_page()

        try:
            # 1. ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
            await page.goto(target_url, wait_until='domcontentloaded', timeout=90000)
            
            # --- å‡¦ç† A: è¨€èªé¸æŠãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ã®å¯¾å¿œ ---
            print("--- å‡¦ç† A: è¨€èªé¸æŠãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ã®ç¢ºèªä¸­ ---")
            try:
                await page.wait_for_selector(LANGUAGE_SELECTOR, timeout=10000)
                await page.click(LANGUAGE_SELECTOR)
                print("âœ… è¨€èªé¸æŠãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ã‚’ã€Œæ—¥æœ¬èªã€ã§é–‰ã˜ã¾ã—ãŸã€‚")
                await page.wait_for_load_state("domcontentloaded", timeout=15000)
            except PlaywrightTimeoutError:
                print("--- å‡¦ç† A: è¨€èªé¸æŠãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ã¯è¡¨ç¤ºã•ã‚Œã¦ã„ã¾ã›ã‚“ã§ã—ãŸã€‚ ---")
            
            # --- å‡¦ç† B: 18æ­³ä»¥ä¸Šç¢ºèªãƒ¢ãƒ¼ãƒ€ãƒ«ã®å¯¾å¿œ ---
            print("--- å‡¦ç† B: 18æ­³ä»¥ä¸Šç¢ºèªãƒ¢ãƒ¼ãƒ€ãƒ«ã®ç¢ºèªä¸­ ---")
            try:
                await page.wait_for_selector(AGE_CONFIRM_SELECTOR, timeout=10000)
                await page.click(AGE_CONFIRM_SELECTOR)
                print("âœ… 18æ­³ä»¥ä¸Šç¢ºèªãƒ¢ãƒ¼ãƒ€ãƒ«ã‚’ã€Œã¯ã„ã€ã§é–‰ã˜ã¾ã—ãŸã€‚")
                await page.wait_for_load_state("domcontentloaded", timeout=15000)
            except PlaywrightTimeoutError:
                print("--- å‡¦ç† B: 18æ­³ä»¥ä¸Šç¢ºèªãƒ¢ãƒ¼ãƒ€ãƒ«ã¯è¡¨ç¤ºã•ã‚Œã¦ã„ã¾ã›ã‚“ã§ã—ãŸã€‚ ---")
            # ------------------------------------

            # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚ã®æ„å›³çš„ãªå¾…æ©Ÿ
            print(f"--- å¾…æ©Ÿä¸­: {ACCESS_DELAY}ç§’ ---")
            time.sleep(ACCESS_DELAY)  

            html_content = await page.content()
            await browser.close()
            
            if "403 ERROR" in html_content or "ã‚¢ã‚¯ã‚»ã‚¹ãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¾ã—ãŸ" in html_content:
                print("--- ğŸš¨ ã‚¢ã‚¯ã‚»ã‚¹ãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¦ã„ã¾ã™ã€‚ ---")
                return None
                
            print("âœ… Playwrightã«ã‚ˆã‚‹ã‚¢ã‚¯ã‚»ã‚¹æˆåŠŸï¼HTMLãƒ‡ãƒ¼ã‚¿ã‚’å—ä¿¡ã—ã¾ã—ãŸã€‚")
            return html_content

        except Exception as e:
            await browser.close()
            print(f"--- âš ï¸ Playwright ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e} ---")
            return None

# ==========================================================
# â˜…è¿½åŠ â˜… ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ãƒ¼ã‚¹ï¼ˆæŠ½å‡ºï¼‰å‡¦ç†
# ==========================================================
def parse_html_for_products(html_content: str):
    """
    DLsiteã®æ–°ç€ä½œå“ä¸€è¦§HTMLã‹ã‚‰ã€ä½œå“åã¨URLã‚’æŠ½å‡ºã—ã¾ã™ã€‚
    """
    print("\n--- ãƒ‡ãƒ¼ã‚¿ã‚’HTMLã‹ã‚‰æŠ½å‡ºä¸­ ---")
    soup = BeautifulSoup(html_content, 'html.parser')
    products = []

    # DLsiteã®ä½œå“ä¸€è¦§ã®ã‚¿ã‚¤ãƒˆãƒ«ãƒªãƒ³ã‚¯ã®CSSã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
    # div.work_1col > dl > dt > a ã¯ã€ä½œå“ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒªãƒ³ã‚¯ã‚’ä¿æŒã™ã‚‹è¦ç´ ã§ã™ã€‚
    product_links = soup.select('#search_result div.work_1col > dl > dt > a') 
    
    if not product_links:
        print("--- âš ï¸ ä½œå“ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒªãƒ³ã‚¯ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æœ¬æ—¥ã®æ–°ç€ãŒãªã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚ ---")
        
    for link in product_links:
        title = link.get_text(strip=True)
        # URLã¯å¸¸ã«çµ¶å¯¾URLã§å–å¾—ã§ãã‚‹
        url = link.get('href')
        
        products.append({
            'title': title,
            'url': url
        })

    print(f"âœ… **{len(products)}ä»¶**ã®ä½œå“ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚")
    return products


# ==========================================================
# å®Ÿè¡Œãƒ¡ã‚¤ãƒ³å‡¦ç†
# ==========================================================
def main():
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã« '--head' ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    headless_mode = '--head' not in sys.argv
    
    print(f"**å®Ÿè¡Œæ—¥ä»˜**: {TODAY_DATE_STR}")
    target_url = HOME_URL
    
    # éåŒæœŸé–¢æ•°ã‚’å®Ÿè¡Œã—ã€HTMLãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    html_data = asyncio.run(scrape_dlsite_new_products(target_url, headless_mode))
    
    if html_data:
        # HTMLãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã™ (ãƒ‡ãƒãƒƒã‚°ç”¨)
        try:
            with open(OUTPUT_FILENAME, "w", encoding="utf-8") as f:
                f.write(html_data)
            # HTMLã®é•·ã•ã‚„ã‚¿ã‚¤ãƒˆãƒ«ã¯Playwrightãƒ‘ãƒ¼ãƒˆã§å‡ºåŠ›æ¸ˆã¿ã®ãŸã‚çœç•¥
        except Exception as file_error:
             print(f"--- âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼: {file_error} ---")

        # ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ãƒ¼ã‚¹ï¼ˆæŠ½å‡ºï¼‰ã‚’å®Ÿè¡Œ
        extracted_products = parse_html_for_products(html_data)
        
        # æŠ½å‡ºçµæœã®è¡¨ç¤º
        print("\n--- æŠ½å‡ºã•ã‚ŒãŸä½œå“ãƒ‡ãƒ¼ã‚¿ï¼ˆä¸Šä½5ä»¶ï¼‰ ---")
        if extracted_products:
            for i, product in enumerate(extracted_products[:5]):
                print(f"[{i+1}] ã‚¿ã‚¤ãƒˆãƒ«: **{product['title']}**")
                print(f"    URL: {product['url']}")
            if len(extracted_products) > 5:
                print(f"  ...ä»– {len(extracted_products) - 5}ä»¶")
        else:
             print("æŠ½å‡ºã§ãã‚‹ä½œå“ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            
        print("--------------------------------------")

    else:
        print("ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ãŸãŸã‚ã€å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")


if __name__ == "__main__":
    if os.name == 'nt':
        try:
            asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
        except AttributeError:
            pass
            
    main()