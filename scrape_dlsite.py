import asyncio
import time
import os
import sys
from datetime import datetime
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError
from bs4 import BeautifulSoup
# ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³å‡¦ç†ã®ãŸã‚ã«pytzã‚’è¿½åŠ ï¼ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ã§ã™: pip install pytzï¼‰
try:
    import pytz
except ImportError:
    print("pytz not found. Please install it using: pip install pytz")
    sys.exit(1)

# ==========================================================
# âš™ï¸ è¨­å®šå€¤
# ==========================================================
ACCESS_DELAY = 5 # ç§’: ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚ã®æœ€çµ‚å¾…æ©Ÿæ™‚é–“
MAX_ITEMS_TO_SCRAPE = 10 # æŠ½å‡ºã™ã‚‹ä½œå“ã®æœ€å¤§ä»¶æ•°

# DLsiteã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼å®šç¾©ï¼ˆPlaywrightç”¨ï¼‰
LANGUAGE_SELECTOR_XPATH = '//div[@class="adult_check_box _adultcheck type_lang_select"]//a[text()="æ—¥æœ¬èª"]' 
AGE_CONFIRM_SELECTOR_CSS = '.btn_yes.btn-approval a' 
ILLUST_TAB_SELECTOR_CSS = '.option_tab a:has-text("CGãƒ»ã‚¤ãƒ©ã‚¹ãƒˆ")' 

# ==========================================================
# ğŸš€ Playwrightã«ã‚ˆã‚‹éåŒæœŸã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•°
# ==========================================================
async def scrape_dlsite_new_products(target_url: str, today_date_str: str, headless_mode: bool = True):
    """
    Playwright (Chromium) ã‚’ä½¿ç”¨ã—ã¦DLsiteã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€è¨€èªé¸æŠã€å¹´é½¢ç¢ºèªã€ã‚«ãƒ†ã‚´ãƒªåˆ‡ã‚Šæ›¿ãˆã‚’å‡¦ç†ã—ã¾ã™ã€‚
    """
    print(f"**å®Ÿè¡Œæ—¥ä»˜**: {today_date_str}")
    print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆURL: {target_url}")
    print(f"--- Playwright ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•ä¸­ (Headless: {headless_mode}) ---")

    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=headless_mode,
            args=['--no-sandbox', '--disable-blink-features=AutomationControlled'],
            timeout=90000
        )
        page = await browser.new_page()

        try:
            await page.goto(target_url, wait_until='domcontentloaded', timeout=90000)
            
            # --- å‡¦ç† A: è¨€èªé¸æŠãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ã®å¯¾å¿œ ---
            print("--- å‡¦ç† A: è¨€èªé¸æŠãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ã®ç¢ºèªä¸­ ---")
            try:
                await page.wait_for_selector(LANGUAGE_SELECTOR_XPATH, timeout=10000) 
                await page.click(LANGUAGE_SELECTOR_XPATH)
                print("âœ… è¨€èªé¸æŠãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ã‚’ã€Œæ—¥æœ¬èªã€ã§é–‰ã˜ã¾ã—ãŸã€‚")
                await page.wait_for_load_state("domcontentloaded", timeout=15000) 
            except PlaywrightTimeoutError:
                print("--- å‡¦ç† A: è¨€èªé¸æŠãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ã¯è¡¨ç¤ºã•ã‚Œã¦ã„ã¾ã›ã‚“ã§ã—ãŸã€‚ ---")
            
            # --- å‡¦ç† B: 18æ­³ä»¥ä¸Šç¢ºèªãƒ¢ãƒ¼ãƒ€ãƒ«ã®å¯¾å¿œ ---
            print("--- å‡¦ç† B: 18æ­³ä»¥ä¸Šç¢ºèªãƒ¢ãƒ¼ãƒ€ãƒ«ã®ç¢ºèªä¸­ ---")
            try:
                await page.wait_for_selector(AGE_CONFIRM_SELECTOR_CSS, timeout=10000)
                await page.click(AGE_CONFIRM_SELECTOR_CSS)
                print("âœ… 18æ­³ä»¥ä¸Šç¢ºèªãƒ¢ãƒ¼ãƒ€ãƒ«ã‚’ã€Œã¯ã„ã€ã§é–‰ã˜ã¾ã—ãŸã€‚")
                await page.wait_for_load_state("domcontentloaded", timeout=15000)
            except PlaywrightTimeoutError:
                print("--- å‡¦ç† B: 18æ­³ä»¥ä¸Šç¢ºèªãƒ¢ãƒ¼ãƒ€ãƒ«ã¯è¡¨ç¤ºã•ã‚Œã¦ã„ã¾ã›ã‚“ã§ã—ãŸã€‚ ---")

            # --- å‡¦ç† C: ã‚«ãƒ†ã‚´ãƒªåˆ‡ã‚Šæ›¿ãˆï¼ˆã€Œã™ã¹ã¦ã€ã‹ã‚‰ã€ŒCGãƒ»ã‚¤ãƒ©ã‚¹ãƒˆã€ã¸ï¼‰ ---
            print("--- å‡¦ç† C: ã‚«ãƒ†ã‚´ãƒªåˆ‡ã‚Šæ›¿ãˆï¼ˆã™ã¹ã¦ -> CGãƒ»ã‚¤ãƒ©ã‚¹ãƒˆï¼‰ ---")
            
            try:
                illust_link = page.locator(ILLUST_TAB_SELECTOR_CSS)
                
                await illust_link.wait_for(state="visible", timeout=10000)
                
                # ã‚¯ãƒªãƒƒã‚¯å®Ÿè¡Œï¼ˆãƒšãƒ¼ã‚¸é·ç§»ãŒç™ºç”Ÿã™ã‚‹ã“ã¨ã‚’è¨±å®¹ï¼‰
                await illust_link.click(timeout=10000) 
                
                # â˜…ä¿®æ­£â˜… ãƒšãƒ¼ã‚¸é·ç§»ãŒç™ºç”Ÿã—ãŸã¨ã¿ãªã—ã€ãƒ­ãƒ¼ãƒ‰çŠ¶æ…‹ã®å®Œäº†ã‚’å¾…æ©Ÿ
                await page.wait_for_load_state("domcontentloaded", timeout=30000) 
                
                print("âœ… ã‚«ãƒ†ã‚´ãƒªã‚’ã€ŒCGãƒ»ã‚¤ãƒ©ã‚¹ãƒˆã€ã«åˆ‡ã‚Šæ›¿ãˆã¾ã—ãŸã€‚ï¼ˆé·ç§»å®Œäº†ï¼‰")
                
            except Exception as e:
                # äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ï¼ˆè¦ç´ ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãªã©ï¼‰ã®å ´åˆã®ã¿è­¦å‘Š
                print(f"--- å‡¦ç† C: âš ï¸ ã‚«ãƒ†ã‚´ãƒªåˆ‡ã‚Šæ›¿ãˆã®æ“ä½œã§äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e.__class__.__name__} ---")
                
            print(f"--- æœ€çµ‚å¾…æ©Ÿä¸­: {ACCESS_DELAY}ç§’ ---")
            time.sleep(ACCESS_DELAY)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›

            html_content = await page.content()
            await browser.close()
                
            print("âœ… Playwrightã«ã‚ˆã‚‹ã‚¢ã‚¯ã‚»ã‚¹æˆåŠŸï¼HTMLãƒ‡ãƒ¼ã‚¿ã‚’å—ä¿¡ã—ã¾ã—ãŸã€‚")
            return html_content


        except Exception as e:
            await browser.close()
            print(f"--- âš ï¸ Playwright ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e} ---")
            return None


# ==========================================================
# ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ãƒ¼ã‚¹ï¼ˆæŠ½å‡ºï¼‰å‡¦ç† - è¤‡æ•°ä»¶å¯¾å¿œ
# ==========================================================
def parse_html_for_products(html_content: str, max_items: int):
    """
    DLsiteã®HTMLã‹ã‚‰ã€ä½œå“åã€URLã€æ¦‚è¦ãªã©ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
    """
    print(f"\n--- ãƒ‡ãƒ¼ã‚¿ã‚’HTMLã‹ã‚‰æŠ½å‡ºä¸­ï¼ˆæœ€å¤§{max_items}ä»¶ï¼‰ ---")
    soup = BeautifulSoup(html_content, 'html.parser')
    products = []

    # ç¢ºå®šã—ãŸä½œå“ã‚¿ã‚¤ãƒˆãƒ«ãƒªãƒ³ã‚¯ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’ä½¿ç”¨
    product_links = soup.select('div.n_worklist_item .work_name a[href*="/product_id/"]') 
    
    if not product_links:
        print("--- âš ï¸ ä½œå“ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒªãƒ³ã‚¯ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ ---")
        return []
        
    for link in product_links[:max_items]: 
        
        # 1. ä½œå“ã‚¿ã‚¤ãƒˆãƒ«
        title = link.get_text(strip=True) or link.get('title')
        
        # 2. ä½œå“URLã¨ID
        url = link.get('href')
        full_url = f"https://www.dlsite.com{url}" if url and url.startswith('/') else url
        product_id = full_url.split('/')[-1].replace('.html', '').replace('.txt', '')
        
        # 3. ä½œå“ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆæ¦‚è¦ï¼‰
        dt_element = link.find_parent('dt')
        description_element = dt_element.find_next_sibling('dd', class_='work_text') if dt_element else None
        description = description_element.get_text(strip=True).replace('\n', ' ') if description_element else 'è©³ç´°ãªèª¬æ˜ãªã—'

        # 4. ä½œè€…å (æŠ½å‡º)
        author_link = link.find_parent('dt').find_next_sibling('dd', class_='maker_name').select_one('a')
        author_name = author_link.get_text(strip=True) if author_link else 'ä¸æ˜'

        products.append({
            'product_id': product_id,
            'title': title,
            'url': full_url,
            'description': description,
            'author': author_name,
        })
        
    print(f"âœ… **{len(products)}ä»¶**ã®ä½œå“ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚")
    return products


# ==========================================================
# ğŸ“ Hugoå‘ã‘Markdownãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆé–¢æ•°
# ==========================================================
def create_hugo_markdown(product: dict, date_info: datetime):
    """
    æŠ½å‡ºã—ãŸä½œå“æƒ…å ±ã‹ã‚‰Hugoå½¢å¼ã®Markdownæ–‡å­—åˆ—ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    """
    # Hugoã®ãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼
    markdown_content = f"""+++
title = "{product['title']}"
date = "{date_info.isoformat()}"
description = "{product['description'][:150]}..."
product_id = "{product['product_id']}"
author = "{product['author']}"
dlsite_url = "{product['url']}"
tags = ["dlscrapes", "cg-illust"]
categories = ["new_releases"]
+++

## {product['title']}

{product['description']}

---

[DLsiteã§è¦‹ã‚‹]({product['url']})

"""
    return markdown_content

# ==========================================================
# ğŸ å®Ÿè¡Œãƒ¡ã‚¤ãƒ³å‡¦ç†
# ==========================================================
def main():
    # å®Ÿè¡Œæ—¥æ™‚ã¨ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã®ç¢ºå®š (JST/Tokyo)
    tokyo_tz = pytz.timezone('Asia/Tokyo')
    now_tokyo = datetime.now(tokyo_tz)
    TODAY_DATE_STR = now_tokyo.strftime('%Y-%m-%d')
    CURRENT_MONTH_STR = now_tokyo.strftime('%Y-%m')

    # åˆæœŸã‚¢ã‚¯ã‚»ã‚¹URLã®å‹•çš„ç”Ÿæˆ
    HOME_URL = f"https://www.dlsite.com/maniax/new/=/date/{TODAY_DATE_STR}/cdate/{CURRENT_MONTH_STR}/show_layout/2"

    headless_mode = '--head' not in sys.argv
    
    # éåŒæœŸé–¢æ•°ã‚’å®Ÿè¡Œã—ã€HTMLãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    html_data = asyncio.run(scrape_dlsite_new_products(HOME_URL, TODAY_DATE_STR, headless_mode))
    
    if html_data:
        # ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ãƒ¼ã‚¹ï¼ˆæŠ½å‡ºï¼‰ã‚’å®Ÿè¡Œ
        extracted_products = parse_html_for_products(html_data, MAX_ITEMS_TO_SCRAPE)
        
        # æŠ½å‡ºçµæœã®è¡¨ç¤ºã¨Markdownç”Ÿæˆ
        print("\n--- æŠ½å‡ºã•ã‚ŒãŸä½œå“ãƒ‡ãƒ¼ã‚¿ã¨Markdownç”Ÿæˆ ---")
        if extracted_products:
            # Markdownå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æº–å‚™
            output_dir = "content/dlsite_new_releases"
            os.makedirs(output_dir, exist_ok=True)

            for product in extracted_products:
                # æŠ½å‡ºçµæœã®è¡¨ç¤º
                print(f"  ã‚¿ã‚¤ãƒˆãƒ«: **{product['title']}** (ID: {product['product_id']})")
                
                # Markdownç”Ÿæˆ
                markdown_content = create_hugo_markdown(product, now_tokyo)
                
                # ãƒ•ã‚¡ã‚¤ãƒ«åã®æ±ºå®š: RJxxxxx.md å½¢å¼
                filename = os.path.join(output_dir, f"{product['product_id']}.md")
                
                try:
                    with open(filename, "w", encoding="utf-8") as f:
                        f.write(markdown_content)
                    print(f"  âœ… Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {filename}")
                except Exception as file_error:
                    print(f"  --- âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼: {file_error} ---")
        else:
             print("æŠ½å‡ºã§ãã‚‹ä½œå“ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            
        print("--------------------------------------")


if __name__ == "__main__":
    if os.name == 'nt':
        try:
            asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
        except AttributeError:
            pass
            
    main()