import requests
from bs4 import BeautifulSoup
import json
import os
import time
from datetime import date
import re

# --- 1. å®šæ•°è¨­å®š ---
# å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (Hugoã®ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª)
DATA_OUTPUT_PATH = 'data/dlsite_new.json'

# --- 2. URLå‹•çš„ç”Ÿæˆ ---
def generate_target_url():
    """å®Ÿè¡Œæ—¥å½“æ—¥ã®DLsiteæ–°ä½œä¸€è¦§URLã‚’ç”Ÿæˆã™ã‚‹ (YYYY-MM-DDå½¢å¼)"""
    today = date.today().strftime("%Y-%m-%d") 
    url = f'https://www.dlsite.com/maniax/new/=/date/{today}/'
    print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆURL: {url}")
    return url

# --- 3. HTMLã®å–å¾— ---
def fetch_html(url, delay):
    """æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰HTMLã‚’å–å¾—ã—ã€è² è·å¯¾ç­–ã¨ã—ã¦é…å»¶ã‚’å…¥ã‚Œã‚‹"""
    time.sleep(delay)  # è² è·å¯¾ç­–ã®ãŸã‚ã®é…å»¶
    try:
        # User-Agentã‚’è¨­å®šï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ã‹ã‚‰ã®ã‚¢ã‚¯ã‚»ã‚¹ã«è¦‹ã›ã‹ã‘ã‚‹ï¼‰
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status() # 200ç•ªå°ä»¥å¤–ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ãªã‚‰ä¾‹å¤–ç™ºç”Ÿ
        return response.text
    except requests.exceptions.RequestException as e:
        print(f"--- âš ï¸ Error fetching URL {url}: {e}")
        return None

# --- 4. ç¬¬1æ®µéš: ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰ä½œå“IDãƒªã‚¹ãƒˆã‚’æŠ½å‡º ---
def get_product_ids(html_content):
    """ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰RJã‚³ãƒ¼ãƒ‰(ä½œå“ID)ã®ãƒªã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹"""
    if not html_content:
        return []

    soup = BeautifulSoup(html_content, 'html.parser')
    ids = []
    
    # æ–°ä½œä¸€è¦§ãƒšãƒ¼ã‚¸ã®ä½œå“ãƒªãƒ³ã‚¯è¦ç´ ã‹ã‚‰RJã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
    # ã‚»ãƒ¬ã‚¯ã‚¿ã¯DLsiteã®HTMLæ§‹é€ ã«åˆã‚ã›ã¦ã„ã¾ã™
    product_links = soup.select('div.work_item a.work_link') 
    
    for link in product_links:
        href = link.get('href')
        if href and 'product_id' in href:
            # URLã‹ã‚‰ RJxxxxxx ã®éƒ¨åˆ†ã‚’æ­£è¦è¡¨ç¾ã§æŠ½å‡º
            match = re.search(r'product_id/([a-zA-Z0-9]+)\.html', href)
            if match:
                ids.append(match.group(1))
    
    unique_ids = list(set(ids))
    print(f"--- âœ… ä¸€è¦§ã‹ã‚‰ {len(unique_ids)} ä»¶ã®ä½œå“IDã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚")
    return unique_ids

# --- 5. ç¬¬2æ®µéš: è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º ---
def scrape_product_details(product_id):
    """å€‹åˆ¥ã®ä½œå“è©³ç´°ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹"""
    detail_url = f'https://www.dlsite.com/maniax/work/=/product_id/{product_id}.html'
    
    # âš ï¸ è² è·å¯¾ç­–ã¨ã—ã¦10ç§’é…å»¶ âš ï¸
    html = fetch_html(detail_url, delay=10) 
    
    if not html:
        return None

    soup = BeautifulSoup(html, 'html.parser')
    data = {'id': product_id, 'url': detail_url}

    try:
        # ã‚¿ã‚¤ãƒˆãƒ«
        data['title'] = soup.select_one('#work_name a').text.strip()
        
        # ä½œè€…/ã‚µãƒ¼ã‚¯ãƒ«å
        maker_tag = soup.select_one('#work_maker a.maker_name') 
        data['maker'] = maker_tag.text.strip() if maker_tag else 'ä½œè€…æƒ…å ±ãªã—'

        # ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒURLï¼ˆãƒ¡ã‚¤ãƒ³ç”»åƒï¼‰
        img_tag = soup.select_one('#work_image img')
        # data-srcå±æ€§ãŒã‚ã‚Œã°ãã‚Œã‚’å„ªå…ˆï¼ˆé…å»¶èª­ã¿è¾¼ã¿å¯¾ç­–ï¼‰
        data['image_url'] = img_tag.get('data-src', img_tag.get('src')) if img_tag else ''

        # ä¾¡æ ¼
        # ä¾¡æ ¼æƒ…å ±ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ã‚»ãƒ¬ã‚¯ã‚¿
        price_text = soup.select_one('.work_detail_area .price').text.strip() if soup.select_one('.work_detail_area .price') else 'ä¾¡æ ¼æƒ…å ±ãªã—'
        data['price_text'] = price_text

        # ã‚¿ã‚°/ã‚¸ãƒ£ãƒ³ãƒ«
        tags = []
        # work_outlineå†…ã®ã‚¿ã‚°ã‚„ã‚¸ãƒ£ãƒ³ãƒ«ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
        category_table = soup.select_one('#work_outline')
        if category_table:
            # ã‚¸ãƒ£ãƒ³ãƒ«ã€å±æ€§ãªã©ã®aã‚¿ã‚°ã‚’ã™ã¹ã¦å–å¾—
            tags = [a.text.strip() for a in category_table.select('a[href*="/genre/"], a[href*="/attribute/"]')]

        data['tags'] = tags
        
        print(f"--- ğŸ“Š æˆåŠŸ: {data['id']} - {data['title'][:20]}...")
        return data

    except AttributeError:
        print(f"--- âŒ ã‚¨ãƒ©ãƒ¼: {product_id} - å¿…è¦ãªæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return None
    except Exception as e:
        print(f"--- âŒ ã‚¨ãƒ©ãƒ¼: {product_id} - å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None

# --- 6. ãƒ‡ãƒ¼ã‚¿å‡ºåŠ› ---
def save_to_json(data):
    """ãƒ‡ãƒ¼ã‚¿ã‚’JSONå½¢å¼ã§Hugoã®ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã™ã‚‹"""
    # dataãƒ•ã‚©ãƒ«ãƒ€ãŒãªã‘ã‚Œã°ä½œæˆ
    os.makedirs(os.path.dirname(DATA_OUTPUT_PATH), exist_ok=True)
    
    # Hugoã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã«åˆã‚ã›ã¦ãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´
    output_data = {
        "products": data 
    }
    
    with open(DATA_OUTPUT_PATH, 'w', encoding='utf-8') as f:
        # JSONã‚’èª­ã¿ã‚„ã™ãï¼ˆã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆä»˜ãï¼‰ã§ä¿å­˜
        json.dump(output_data, f, ensure_ascii=False, indent=4)
    print(f"\nâœ… å…¨å‡¦ç†å®Œäº†: æ­£å¸¸ã« {len(data)} ä»¶ã®ä½œå“ãƒ‡ãƒ¼ã‚¿ã‚’ {DATA_OUTPUT_PATH} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

# --- ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ ---
if __name__ == "__main__":
    # 1. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆURLã‚’ç”Ÿæˆã—ã€HTMLã‚’å–å¾—
    target_url = generate_target_url()
    list_html = fetch_html(target_url, delay=5) # ä¸€è¦§ã‚¢ã‚¯ã‚»ã‚¹æ™‚ã¯5ç§’å¾…æ©Ÿ

    if not list_html:
        print("\n--- ğŸ›‘ å‡¦ç†ä¸­æ–­: ä¸€è¦§ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
    else:
        # 2. ä½œå“IDãƒªã‚¹ãƒˆã‚’å–å¾—
        product_ids = get_product_ids(list_html)
        
        if not product_ids:
            print("\n--- â„¹ï¸ æƒ…å ±: æœ¬æ—¥æ–°ä½œã®æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        else:
            final_products = []
            
            # 3. å„ä½œå“ã®è©³ç´°ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° (10ç§’é…å»¶)
            for i, pid in enumerate(product_ids):
                print(f"å‡¦ç†ä¸­ ({i+1}/{len(product_ids)}): {pid} ã®è©³ç´°ã‚’å–å¾—...")
                detail = scrape_product_details(pid)
                if detail:
                    final_products.append(detail)
            
            # 4. JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
            save_to_json(final_products)